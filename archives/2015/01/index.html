<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Archives: 2015/1 | Sean&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sean's walk step by step">
<meta property="og:type" content="website">
<meta property="og:title" content="Sean's blog">
<meta property="og:url" content="http://blog.xiaolud.com/archives/2015/01/">
<meta property="og:site_name" content="Sean's blog">
<meta property="og:description" content="Sean's walk step by step">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sean's blog">
<meta name="twitter:description" content="Sean's walk step by step">

  
    <link rel="alternative" href="/atom.xml" title="Sean&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/image/favicon.ico">
  
  <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link href="//libs.useso.com/js/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" href="/css/style.css" type="text/css">

  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Sean&#39;s blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Being-towards-death</a>
        </h2>
      
    </div>
    <div id="header-menu">
      <nav id="main-nav">
        <ul>
        
          <li><a href="/"><i class="fa fa-home icon-setting"></i></a></li>
        
          <li><a href="/archives"><i class="fa fa-archive icon-setting"></i></a></li>
        
          <li><a href="/about"><i class="fa fa-user icon-setting"></i></a></li>
        
        
          <li><a href="/atom.xml"><i class="fa fa-rss %> icon-setting"></i></a></li>
        
        </ul>
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-spider-4th-doubantop100" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/29/spider-4th-doubantop100/" class="article-date">
  <time datetime="2015-01-29T14:00:00.000Z" itemprop="datePublished">Jan 29 2015</time>
</a>
    
  </div>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/29/spider-4th-doubantop100/">Python爬虫基础4-豆瓣top100</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>看了下，很多人都有写如何爬豆瓣，遂当做一个学习课题，写出来看看啊。本文主要分一下几个部分：</p>
<blockquote>
<p>分析豆瓣top页面，构建程序结构<br>依次写出代码<br>整理代码</p>
</blockquote>
<h1 id="分析豆瓣top页面，构建程序结构">分析豆瓣top页面，构建程序结构</h1>
<p>1.首先打开网页<a href="http://movie.douban.com/top250?start，也就是top页面" target="_blank" rel="external">http://movie.douban.com/top250?start，也就是top页面</a><br>然后试着点击到top100的页面，注意带top100的链接依次为</p>
<pre><code>http:<span class="comment">//movie.douban.com/top250?start=0</span>
http:<span class="comment">//movie.douban.com/top250?start=25</span>
http:<span class="comment">//movie.douban.com/top250?start=50</span>
http:<span class="comment">//movie.douban.com/top250?start=75</span>
</code></pre><p>2.然后通过查看源码，发现电影名的代码如下：</p>
<pre><code><span class="tag">&lt;<span class="title">span</span> <span class="attribute">class</span>=<span class="value">"title"</span>&gt;</span>肖申克的救赎<span class="tag">&lt;/<span class="title">span</span>&gt;</span>
<span class="tag">&lt;<span class="title">span</span> <span class="attribute">class</span>=<span class="value">"title"</span>&gt;</span>&amp;nbsp;/&amp;nbsp;The Shawshank Redemption<span class="tag">&lt;/<span class="title">span</span>&gt;</span>
</code></pre><p>如图，因为有一些英文名等描述，通过正则抓取有些干扰，可能还需要后续过滤。</p>
<p>根据以上信息，此程序主要分以下3个步骤：</p>
<blockquote>
<p>构建url地址池<br>抓取top100电影名称<br>依次打印输出</p>
</blockquote>
<h1 id="依次写出代码">依次写出代码</h1>
<p>1.构建url地址池。代码如下：</p>
<pre><code><span class="built_in">import</span> urllib2
<span class="built_in">import</span> re
<span class="comment"># ----------确定url地址池------------</span>
<span class="variable">pre_url =</span> 'http://movie.douban.com/top250?<span class="variable">start=</span>'
<span class="variable">top_urls =</span> []
<span class="comment"># 因为top100，每页25部电影，故为4页,从零开始</span>
for num <span class="keyword">in</span> range(<span class="number">4</span>):
    top_urls.append(pre_url + str(num * <span class="number">25</span>))
</code></pre><p>2.抓取top100电影名称</p>
<pre><code><span class="comment"># ------------抓取top100电影名称----------</span>
top_content = []
top_tag = re.compile(<span class="string">r'&lt;span class="title"&gt;(.+?)&lt;/span&gt;'</span>)
<span class="keyword">for</span> url <span class="keyword">in</span> top_urls:
    content = urllib2.urlopen(url).read()
    pre_content = re.findall(top_tag, content)
    <span class="comment"># 过滤不符合条件的list，得到最后的top100的list</span>
    <span class="keyword">for</span> item <span class="keyword">in</span> pre_content:
        <span class="keyword">if</span> item.find(<span class="string">'&amp;nbsp'</span>) == -<span class="number">1</span>:
            top_content.append(item)
</code></pre><p>3.打印输出</p>
<pre><code>top_num = <span class="number">1</span>
<span class="keyword">for</span> <span class="keyword">item</span> <span class="operator">in</span> top_content:
    print <span class="string">'Top'</span> + str(top_num) + <span class="string">'   '</span> + <span class="keyword">item</span>
    top_num += <span class="number">1</span>
</code></pre><h1 id="整理代码">整理代码</h1>
<p>我还是python新手，还没有太多的pythonic思想，也没有代码优化技巧，只能说是整理。<br>其次，个人习惯，在简单的代码里面我还是喜欢少用函数，尽量不隐藏代码的逻辑。<br>以下代码请参考，并欢迎提意见，希望得到大家的意见，谢谢！<br>整理后的代码如下：</p>
<pre><code><span class="comment"># coding=utf-8</span>
<span class="string">'''
本代码为自动抓取豆瓣top100电影代码
@pre_url  url地址前缀，在这里为http://movie.douban.com/top250?start=
@top_urls url地址池
@top_tag  为抓取电影名正则表达式
'''</span>

<span class="keyword">import</span> urllib2
<span class="keyword">import</span> re

pre_url = <span class="string">'http://movie.douban.com/top250?start='</span>
top_urls = []
top_tag = re.compile(<span class="string">r'&lt;span class="title"&gt;(.+?)&lt;/span&gt;'</span>)
top_content = []
top_num = <span class="number">1</span>

<span class="comment"># ----------确定url地址池------------</span>
<span class="comment"># 因为top100，每页25部电影，故为4页,从零开始</span>
<span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">4</span>):
    top_urls.append(pre_url + str(num * <span class="number">25</span>))


<span class="comment"># ------------抓取top100电影名称，并打印输出----------</span>
top_tag = re.compile(<span class="string">r'&lt;span class="title"&gt;(.+?)&lt;/span&gt;'</span>)
<span class="keyword">for</span> url <span class="keyword">in</span> top_urls:
    content = urllib2.urlopen(url).read()
    pre_content = re.findall(top_tag, content)
    <span class="comment"># 过滤并打印输出</span>
    <span class="keyword">for</span> item <span class="keyword">in</span> pre_content:
        <span class="keyword">if</span> item.find(<span class="string">'&amp;nbsp'</span>) == -<span class="number">1</span>:
            <span class="keyword">print</span> <span class="string">'Top'</span> + str(top_num) + <span class="string">'   '</span> + item
            top_num += <span class="number">1</span>
</code></pre>
      
    </div>
    <footer class="article-footer">
      
        <a href="http://blog.xiaolud.com/2015/01/29/spider-4th-doubantop100/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/douban/">douban</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spider/">spider</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-splider-3rd-bs4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/24/splider-3rd-bs4/" class="article-date">
  <time datetime="2015-01-24T03:14:48.000Z" itemprop="datePublished">Jan 24 2015</time>
</a>
    
  </div>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/24/splider-3rd-bs4/">Python爬虫基础3-BeautifulSoup4</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在前一节我们主要讲了如图抓取受限制网站,这一节将会介绍爬虫神兵利器BeautifulSoup4.<br>主要包含以下内容:</p>
<blockquote>
<p>安装BeautifulSoup4<br>小试牛刀<br>总结</p>
</blockquote>
<h1 id="1-安装BeautifulSoup4">1.安装<strong>BeautifulSoup4</strong></h1>
<p>easy_install安装方式,easy_install需要提前安装</p>
<pre><code><span class="title">easy_install</span> beautifulsoup4
</code></pre><p>pip安装方式,pip也需要提前安装.此外PyPi中还有一个名字是 BeautifulSoup 的包,那是 Beautiful Soup3 的发布版本.在这里不建议安装.</p>
<pre><code>pip <span class="operator"><span class="keyword">install</span> beautifulsoup4</span>
</code></pre><p>Debain或ubuntu安装方式</p>
<pre><code>apt-<span class="keyword">get</span> install Python-bs4
</code></pre><p>你也可以通过源码安装,<a href="http://www.crummy.com/software/BeautifulSoup/bs4/download/4.0/" title="下载BS4源码" target="_blank" rel="external">下载BS4源码</a></p>
<pre><code>Python setup.<span class="keyword">py</span> install
</code></pre><h1 id="2-小试牛刀">2.小试牛刀</h1>
<p>老样子,直接上代码</p>
<pre><code><span class="comment"># coding=utf-8</span>
<span class="string">'''
@通过BeautifulSoup下载百度贴吧图片
'''</span>
<span class="keyword">import</span> urllib
<span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup
url = <span class="string">'http://tieba.baidu.com/p/3537654215'</span>

<span class="comment"># 下载网页</span>
html = urllib.urlopen(url)
content = html.read()
html.close()

<span class="comment"># 使用BeautifulSoup匹配图片</span>
html_soup = BeautifulSoup(content)
<span class="comment"># 图片代码我们在[Python爬虫基础1--urllib]( http://blog.xiaolud.com/2015/01/22/spider-1st/ "Python爬虫基础1--urllib")里面已经分析过了</span>
<span class="comment"># 相较通过正则表达式去匹配,BeautifulSoup提供了一个更简单灵活的方式</span>
all_img_links = html_soup.findAll(<span class="string">'img'</span>, class_=<span class="string">'BDE_Image'</span>)

<span class="comment"># 接下来就是老生常谈的下载图片</span>
img_counter = <span class="number">1</span>
<span class="keyword">for</span> img_link <span class="keyword">in</span> all_img_links:
    img_name = <span class="string">'%s.jpg'</span> % img_counter
    urllib.urlretrieve(img_link[<span class="string">'src'</span>], img_name)
    img_counter += <span class="number">1</span>
</code></pre><p>很简单,代码注释里面已经解释的很清楚了.BeautifulSoup提供了一个更简单灵活的方式,去分析网站源码,更快获取图片link.</p>
<h1 id="3-总结">3.总结</h1>
<pre><code>未完待续
</code></pre>
      
    </div>
    <footer class="article-footer">
      
        <a href="http://blog.xiaolud.com/2015/01/24/splider-3rd-bs4/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BeautifulSoup4/">BeautifulSoup4</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bs4/">bs4</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spider/">spider</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-spider-2nd-urllib2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/23/spider-2nd-urllib2/" class="article-date">
  <time datetime="2015-01-23T12:14:48.000Z" itemprop="datePublished">Jan 23 2015</time>
</a>
    
  </div>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/23/spider-2nd-urllib2/">Python爬虫基础2--urllib2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一节我们讲解如何抓取网页和下载图片,在下一节里面我们会讲解如何抓取有限制抓取的网站<br>首先,我们依然用我们上一节课的方法去抓取一个大家都用来举例的网站<blog.cndn.net>,本文主要分以下几个部分:</blog.cndn.net></p>
<blockquote>
<p>1.抓取受限网页<br>2.对代码进行一些优化</p>
</blockquote>
<h1 id="1-抓取受限网页">1.抓取受限网页</h1>
<p>首先使用我们上一节学到的知识测试一下:</p>
<pre><code><span class="horizontal_rule">'''</span>
@本程序用来抓取blog.csdn.net网页
<span class="horizontal_rule">'''</span>
import urllib

url = "http://blog.csdn.net/FansUnion"
html = urllib.urlopen(url)
#getcode()方法为返回Http状态码
print html.getcode()
html.close()
#输出
403
</code></pre><p>此处我们的输出为403,代表拒绝访问;同理200表示请求成功完成;404表示网址未找到.<br>可见csdn已做了相关屏蔽,通过第一节的方法是无法获取网页,在这里我们需要启动一个新的库:<strong>urllib2</strong><br>但是我们也看到浏览器可以发那个文,是不是我们模拟浏览器操作,就可以获取网页信息.<br>老办法,我们先来看看浏览器是如何提交请求给csdn服务器的.首先简述一下方法:</p>
<blockquote>
<p>打开网页,右键点击,选择”inspect Element”(最下面这一项)<br>点击下面弹起来的框框的<strong>Network</strong>选项卡<br>刷新网页,就可以看到<strong>Network</strong>选项卡抓取了很多信息<br>找到其中一个信息展开,就能看到请求包的Header</p>
</blockquote>
<p><img src="http://7u2i0z.com1.z0.glb.clouddn.com/2015_01_splider_header_request_2nd.gif" alt="header信息查看" title="header信息查看"></p>
<p><strong>以下就是整理后的Header信息</strong></p>
<pre><code>Request <span class="function"><span class="keyword">Method</span>:</span>GET
Host:blog.csdn.net
Referer:http:<span class="comment">//blog.csdn.net/?ref=toolbar_logo</span>
User-Agent:Mozilla/<span class="number">5.0</span> (Macintosh; Intel Mac OS X <span class="number">10</span>_10_1) AppleWebKit/<span class="number">537.36</span> (KHTML, like Gecko) Chrome/<span class="number">38.0</span>.<span class="number">2125.104</span> Safari/<span class="number">537.36</span>
</code></pre><p>然后根据提取的Header信息,利用urllib2的Request方法模拟浏览器向服务器提交请求,代码如下:</p>
<pre><code><span class="comment"># coding=utf-8</span>
<span class="string">'''
@本程序用来抓取受限网页(blog.csdn.net)
@User-Agent:客户端浏览器版本
@Host:服务器地址
@Referer:跳转地址
@GET:请求方法为GET
'''</span>
<span class="keyword">import</span> urllib2

url = <span class="string">"http://blog.csdn.net/FansUnion"</span>

<span class="comment">#定制自定义Header,模拟浏览器向服务器提交请求</span>
req = urllib2.Request(url)
req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'</span>)
req.add_header(<span class="string">'Host'</span>, <span class="string">'blog.csdn.net'</span>)
req.add_header(<span class="string">'Referer'</span>, <span class="string">'http://blog.csdn.net'</span>)
req.add_header(<span class="string">'GET'</span>, url)

<span class="comment">#下载网页html并打印</span>
html = urllib2.urlopen(req)
content = html.read()
<span class="keyword">print</span> content
html.close()
</code></pre><p>呵呵,你限制我,我就跳过你的限制.据说只要浏览器能够访问的,就能够通过爬虫抓取.</p>
<h1 id="2-对代码进行一些优化">2.对代码进行一些优化</h1>
<h3 id="简化提交Header方法">简化提交Header方法</h3>
<p>发现每次写那么多req.add_header对自己来说是一种折磨,有没有什么方法可以只要复制过来就使用.答案是肯定的.</p>
<pre><code><span class="variable">#input</span>:
help(urllib2<span class="built_in">.</span>Request)
<span class="variable">#output</span>(因篇幅关系,只取__init__方法)
__init__(<span class="built_in">self</span>, url, <span class="built_in">data</span><span class="subst">=</span><span class="literal">None</span>, headers<span class="subst">=</span>{}, origin_req_host<span class="subst">=</span><span class="literal">None</span>, unverifiable<span class="subst">=</span><span class="literal">False</span>)
</code></pre><p>通过观察,我们发现headers={},就是说可以以字典的方式提交header信息.那就动手试试咯!!</p>
<pre><code><span class="comment">#只取自定义Header部分代码</span>
<span class="variable">csdn_headers =</span> {
    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36"</span>,
    <span class="string">"Host"</span>: <span class="string">"blog.csdn.net"</span>,
    'Referer': 'http://blog.csdn.net',
    <span class="string">"GET"</span>: url
    }
<span class="variable">req =</span> urllib2.Request(url,<span class="variable">headers=</span>csdn_headers)
</code></pre><p>发现是不是很简单,在这里感谢斯巴达的无私赐教.</p>
<h3 id="提供动态头部信息">提供动态头部信息</h3>
<p>如果按照上述方法进行抓取,很多时候会因为提交信息过于单一,被服务器认为是机器爬虫进行拒绝.<br>那我们是不是有一些更为智能的方法提交一些动态的数据,答案肯定也是肯定的.而且很简单,直接上代码!</p>
<pre><code><span class="string">'''
@本程序是用来动态提交Header信息
@random 动态库,详情请参考&lt;https://docs.python.org/2/library/random.html&gt;
'''</span>

<span class="comment"># coding=utf-8</span>
<span class="keyword">import</span> urllib2
<span class="keyword">import</span> random

url = <span class="string">'http://www.lifevc.com/'</span>

my_headers = [
    <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30; .NET CLR 3.0.04506.648)'</span>,
    <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; InfoPath.1'</span>,
    <span class="string">'Mozilla/4.0 (compatible; GoogleToolbar 5.0.2124.2070; Windows 6.0; MSIE 8.0.6001.18241)'</span>,
    <span class="string">'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)'</span>,
    <span class="string">'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; Sleipnir/2.9.8)'</span>,
    <span class="comment">#因篇幅关系,此处省略N条</span>
]

random_header = random.choice(headers)
<span class="comment"># 可以通过print random_header查看提交的header信息</span>
req = urllib2.Request(url)
req.add_header(<span class="string">"User-Agent"</span>, random_header)
req.add_header(<span class="string">'Host'</span>, <span class="string">'blog.csdn.net'</span>)
req.add_header(<span class="string">'Referer'</span>, <span class="string">'http://blog.csdn.net'</span>)
req.add_header(<span class="string">'GET'</span>, url)
content = urllib2.urlopen(req).read()
<span class="keyword">print</span> content
</code></pre><p>其实很简单,这样我们就完成了对代码的一些优化.<br>这一节我们主要讲了如图抓取受限制网站,下一节将会介绍爬虫神兵利器BeautifulSoup.</p>

      
    </div>
    <footer class="article-footer">
      
        <a href="http://blog.xiaolud.com/2015/01/23/spider-2nd-urllib2/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spider/">spider</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/urllib/">urllib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/urllib2/">urllib2</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-spider-1st-urllib" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/22/spider-1st-urllib/" class="article-date">
  <time datetime="2015-01-22T12:14:48.000Z" itemprop="datePublished">Jan 22 2015</time>
</a>
    
  </div>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/22/spider-1st-urllib/">Python爬虫基础1--urllib</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>学习python完基础,有些迷茫.眼睛一闭,一种空白的窒息源源不断而来.还是缺少练习,遂拿爬虫来练练手.学习完斯巴达python爬虫课程后,将心得整理如下,供后续翻看.整篇笔记主要分以下几个部分:</p>
<blockquote>
<p>1.做一个简单的爬虫程序<br>2.小试牛刀—抓取百度贴吧图片<br>3.总结</p>
</blockquote>
<h1 id="1-做一个简单的爬虫程序">1.做一个简单的爬虫程序</h1>
<h3 id="首先环境描述">首先环境描述</h3>
<blockquote>
<p>Device: Mba 2012 Yosemite 10.10.1<br>Python: python 2.7.9<br>编辑器: Sublime Text 3</p>
</blockquote>
<p>这个没有什么好说的,直接上代码吧!</p>
<pre><code><span class="horizontal_rule">'''</span>
@ urllib为python自带的一个网络库
@ urlopen为urllib的一个方法,用于打开一个连接并抓取网页,
<span class="code">  然后通过read()方法把值赋给read()</span>
<span class="horizontal_rule">'''</span>
import urllib

url = "http://www.lifevc.com"#多嘴两句,为什么要选lifevc呢,主要是最近它很惹我.
html = urllib.urlopen(url)
content = html.read()
html.close()
#可以通过print打印出网页内容
print content
</code></pre><p>很简单,基本上没有可说的,这个也就是python的魅力,几行代码就完成.<br>当然我们仅仅抓取网页,没有实在的价值.接下来我们就开始做一点有意义的事情.</p>
<h1 id="2-小试牛刀">2.小试牛刀</h1>
<h3 id="抓取百度贴吧图片">抓取百度贴吧图片</h3>
<p>其实也很简单,因为要抓取图片,还需要先分析一下网页源代码<br><strong>(这里以知道基本html知识,浏览器以chrome为例)</strong><br>如图,这里简要说下步骤,请参考.</p>
<blockquote>
<p>打开网页,右键点击,选择”inspect Element”(最下面这一项)<br>点击下面弹起来的框框<strong>最左边那个问号</strong>,问号会变成蓝色<br>移动鼠标去点击我们想要抓取的图片(一个萌妹子)<br>如图,我们就可以图片在源码中的位置了</p>
</blockquote>
<p><img src="http://7u2i0z.com1.z0.glb.clouddn.com/2015_01_chrome_url.gif" alt="图片源码分析" title="图片源码分析"></p>
<p>下面将源码相关拷贝出来</p>
<pre><code>&lt;img <span class="variable">class=</span><span class="string">"BDE_Image"</span> <span class="variable">src=</span><span class="string">"http://imgsrc.baidu.com/forum/w%3D580/
sign=3d5aacaab21c8701d6b6b2ee177e9e6e/17a6d439b6003af329aece2e342ac65c1138b6d8.
jpg"</span> <span class="variable">height=</span><span class="string">"840"</span> <span class="variable">width=</span><span class="string">"560"</span> <span class="variable">style=</span><span class="string">"cursor: url(http://tb2.bdstatic.com/tb/
static-pb/img/cur_zin.cur), pointer;"</span>&gt;
</code></pre><p>经分析和对比(这里略掉),基本上可以看到要抓取的图片几个特征:</p>
<blockquote>
<p>1.在img标签下<br>2.在名为BDE_Image的类下面<br>3.图片格式为jpg<br>    正则表达式后续我会更新,请关注</p>
</blockquote>
<p>依照上述判断,直接上代码</p>
<pre><code><span class="string">'''
@本程序用来下载百度贴吧图片
@re 为正则说明库
'''</span>
<span class="keyword">import</span> urllib
<span class="keyword">import</span> re

<span class="comment"># 获取网页html信息</span>
url = <span class="string">"http://tieba.baidu.com/p/2336739808"</span>
html = urllib.urlopen(url)
content = html.read()
html.close()

<span class="comment"># 通过正则匹配图片特征,并获取图片链接</span>
img_tag = re.compile(<span class="string">r'class="BDE_Image" src="(.+?\.jpg)"'</span>)
img_links = re.findall(img_tag, content)

<span class="comment"># 下载图片 img_counter为图片计数器(文件名)</span>
img_counter = <span class="number">0</span>
<span class="keyword">for</span> img_link <span class="keyword">in</span> img_links:
    img_name = <span class="string">'%s.jpg'</span> % img_counter
    urllib.urlretrieve(img_link, <span class="string">"//Users//Sean//Downloads//tieba//%s"</span> %img_name)
    img_counter += <span class="number">1</span>
</code></pre><p><strong>如图,我们就抓取你懂的图片</strong><br>    <img src="http://7u2i0z.com1.z0.glb.clouddn.com/2015_01_tieba.png" alt="百度贴吧抓取图片" title="百度贴吧抓取图片"></p>
<h3 id="3-总结">3.总结</h3>
<p>如上两节,我们就很轻松的就可以网页或者图片.<br>补充一点小技巧,如果遇到不是很明白的库或者方法,可以通过以下方法进行初步了解.</p>
<pre><code>dir(urllib)                     <span class="comment">#查看当前库有哪些方法</span>
<span class="built_in">help</span>(urllib.urlretrieve)        <span class="comment">#查看跟当前方法相关的作用或者参数,官方比较权威</span>
</code></pre><p>或者<a href="https://docs.python.org/2/library/index.html" target="_blank" rel="external">https://docs.python.org/2/library/index.html</a>进项相关搜索.<br>当然百度也可以,但是效率太低.建议使用 <a href="http://xie.lu" target="_blank" rel="external">http://xie.lu</a> 进行相关搜索(你懂了,绝对满意).<br>这一节我们讲解如何抓取网页和下载图片,在下一节里面我们会讲解如何抓取有限制抓取的网站.</p>

      
    </div>
    <footer class="article-footer">
      
        <a href="http://blog.xiaolud.com/2015/01/22/spider-1st-urllib/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spider/">spider</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/urllib/">urllib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/urllib2/">urllib2</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Hexo_blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/19/Hexo_blog/" class="article-date">
  <time datetime="2015-01-18T18:02:32.000Z" itemprop="datePublished">Jan 19 2015</time>
</a>
    
  </div>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/19/Hexo_blog/">Hexo搭建blog__总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一直以来,都有着折腾blog的想法,除了刚出来工作的那段时间,进场在51cto上写些东西.后续就基本上就荒芜了.wordpress买了不少,主机和域名也琢磨了不少,就是压根没写什么东西.而且也越来越感觉到,毕业一年后到现在,自己几乎没有怎么努力做一些事情,进步也是乏善可陈.现在开始换方向了,作为一名高龄程序猿,需要付出的也许更多,也必须坚持.希望这次能写写东西,能写的久一点.那就从搭建blog这个事情上开始做起吧!</p>
<hr>
<p>经过一段时间的折腾,总算搭建起来了Hexo的blog.学习hexo搭建静态博客中遇到些许问题,总结下来,以供后续翻看.</p>
<blockquote>
<p>1.Hexo安装<br>2.小试牛刀<br>3.配置文件及主题(模板文件)</p>
</blockquote>
<h1 id="1-_Hexo安装"><strong>1. Hexo安装</strong></h1>
<h2 id="依赖环境准备">依赖环境准备</h2>
<h5 id="Git安装">Git安装</h5>
<pre><code>Mac: brew <span class="operator"><span class="keyword">install</span> git
Linux (Ubuntu, Debian): sudo apt-<span class="keyword">get</span> <span class="keyword">install</span> git-core
Linux (Fedora, Red Hat, CentOS): sudo yum <span class="keyword">install</span> git-core
Windows: Download &amp; <span class="keyword">install</span> msysgit.</span>
</code></pre><h5 id="Node-js安装">Node.js安装</h5>
<p>-点击<a href="http://nodejs.org/download/" title="nodj官网下载链接" target="_blank" rel="external">Nodj官网下载链接</a>下载对应版本进行安装</p>
<pre><code>mac用户也可以使用brew <span class="operator"><span class="keyword">install</span> node进行安装</span>
</code></pre><h5 id="Github准备(后续会单独整理一篇blog,静候后续更新)">Github准备(后续会单独整理一篇blog,静候后续更新)</h5>
<blockquote>
<p>创建Github账号<br>创建SSH key并添加到Github<br>设置个人信息(link到github上)<br>建立对应仓库名(your_username.github.io)</p>
</blockquote>
<h5 id="Mac用户还需要安装Xcode和Xcode命令行工具">Mac用户还需要安装Xcode和Xcode命令行工具</h5>
<h2 id="安装Hexo">安装Hexo</h2>
<p>当前面先决条件都安装好了之后,运行下列命令安装Hexo</p>
<pre><code>npm <span class="operator"><span class="keyword">install</span> hexo -g</span>
</code></pre><p>初始化博客目录</p>
<pre><code>hexo init <span class="tag">&lt;<span class="title">folder</span>&gt;</span>
</code></pre><p>生成静态页面</p>
<pre><code>hexo <span class="keyword">generate</span>
</code></pre><p>本地启动,然后在浏览器里面输入<a href="http://127.0.0.1:4000" target="_blank" rel="external">http://127.0.0.1:4000</a>就可以看到效果.(可以使用’ctrl+c’关闭Server).</p>
<pre><code>hexo <span class="keyword">server</span>
</code></pre><p>上传站点到Github</p>
<blockquote>
<p>首先需要修改配置文件<strong>./_config.yml</strong>,如下:</p>
</blockquote>
<pre><code><span class="label">deploy:</span>
<span class="label">type:</span> github
<span class="label">repository:</span> https://github.com/yourname/youname.github.io.git
<span class="label">branch:</span> master <span class="preprocessor">#一般填写master即可</span>
</code></pre><blockquote>
<p>然后运行以下命令,稍等片刻,可以通过<a href="http://yourname.github.io" target="_blank" rel="external">http://yourname.github.io</a>访问你上传的demo网站.绑定指定域名会在后续博文中做出说明.</p>
</blockquote>
<pre><code><span class="title">hexo</span> deploy
</code></pre><p>到此为止,已经完成Hexo的初步安装,并上传到Github.<br>下面我们将介绍如何新建一篇blog和页面,并完成自定义域名的访问.</p>
<h1 id="2-小试牛刀"><strong>2.小试牛刀</strong></h1>
<h3 id="新建文章,也就是新建一篇blog">新建文章,也就是新建一篇blog</h3>
<pre><code>hexo <span class="keyword">new</span> <span class="string">"blog_name"</span>
</code></pre><p>运行完命令后会在./source/<em>posts下面生成 blog<em>name.md文件,就可以在vim或者其他任意编辑器里面尽情的进行创作了.<br>__blog头部解析:</em></em></p>
<pre><code><span class="attribute">title</span>: <span class="string">             #当前blog名称,比如title: Hexo搭建blog_总结</span>
<span class="attribute">tags</span>: <span class="string">[]            #blog标签,比如github,hexo</span>
<span class="attribute">date</span>: <span class="string">2015-01-23 15:06:20</span>
<span class="attribute">categories</span>: <span class="string">         #文章归类,比如小技巧,博客搭建</span>
</code></pre><p>编辑blog是请遵循MarkDown语法,详情可以参考<a href="http://wowubuntu.com/markdown/" title="MarkDown语法" target="_blank" rel="external">MarkDown语法</a></p>
<h3 id="新建页面,比如新建关于,友情链接等页面">新建页面,比如新建关于,友情链接等页面</h3>
<pre><code>hexo <span class="keyword">new</span> page <span class="string">"page_name"</span>
</code></pre><p>然后可以在<a href="http://127.0.0.1:4000" target="_blank" rel="external">http://127.0.0.1:4000</a>查看最终修改结果,确定无误后.重新通过以下步骤<strong>部署到Github</strong>.</p>
<pre><code><span class="title">hexo</span> generater  <span class="comment">#重新生成静态页面,静态网站更新需要全站更新</span>
hexo deploy     <span class="comment">#将刚才新生成的站点同步到Github上</span>
</code></pre><h3 id="绑定自定义域名">绑定自定义域名</h3>
<p>购买域名:推荐使用Godaddy.全球最大的域名提供商,关键是它还支持支付宝.详情略.<br>配置DNS,这里以dnspod为例,详情如图:</p>
<p><img src="http://7u2i0z.com1.z0.glb.clouddn.com/2015_01_dnspod.png" alt="dnspod" title="dnspod"></p>
<p>设置CNAME,玩过Github Page的应该知道,只需要手动在yourname.github.io根页面下新建一个CNAME的文件.这里很重要,因为静态网站更新是全站整体更新的.如果手动添加的会在更新时被删除掉,导致配置失效.<br>在这里,我们需要在<strong>./.source/</strong>目录下手动新建一个<strong>CNAME</strong>文件,写入指定的域名.</p>
<pre><code>blog.xiaolud.<span class="keyword">com</span>    #如上图,我们这里填入&lt;blog.xiaolud.<span class="keyword">com</span>&gt;即可.
</code></pre><p>稍等片刻,我们就能够通过指定的域名访问我们的blog了.<br>到此,我们就完成了blog的搭建,并学习了如何写一篇blog,新建一个新的页面.下一节将介绍一些基础的配置,主题的更换以及评论的开启.</p>
<h1 id="3-配置文件及主题">3.配置文件及主题</h1>
<h3 id="网站目录">网站目录</h3>
<pre><code>├── _config.yml
├── package.json
├── scaffolds
├── scripts
├── source
<span class="string">|   ├── _drafts</span>
<span class="string">|   └── _posts</span>
└── themes
</code></pre><h3 id="_config-yml">_config.yml</h3>
<p>Hexo 主要配置文件为<strong>./_config.yml</strong>,我们在第一节里面已有说明,这里只列出需要配置的选项.<br>不多,详情请参考<a href="http://hexo.io/docs/configuration.html,&quot;hexo官方配置&quot;" target="_blank" rel="external">hexo官方配置</a>.</p>
<pre><code><span class="comment"># Site</span>
<span class="attribute">title</span>: Sean<span class="string">'s blog #网站标题
subtitle: Being-towards-death#网站小标题
description: Sean'</span>s walk step <span class="keyword">by</span> step<span class="comment">#网站描述</span>
<span class="attribute">author</span>: Sean<span class="comment"># yourname</span>
<span class="attribute">email</span>: xiaoxxxx<span class="property">@xxxx</span>.com<span class="comment"># your mail</span>
<span class="attribute">language</span>: zh-CN<span class="comment">#language</span>

<span class="comment"># URL</span>
<span class="comment">## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'</span>
<span class="attribute">url</span>: <span class="attribute">http</span>:<span class="regexp">//</span>blog.xiaolud.com <span class="comment">#你的域名</span>
<span class="attribute">root</span>: /
<span class="attribute">permalink</span>: :year<span class="regexp">/:month/</span>:day<span class="regexp">/:title/</span>
<span class="attribute">tag_dir</span>: tags
<span class="attribute">archive_dir</span>: archives
<span class="attribute">category_dir</span>: categories
<span class="attribute">code_dir</span>: downloads/code
<span class="attribute">permalink_defaults</span>:

<span class="comment"># Disqus#hexo默认支持Disqus</span>
<span class="attribute">disqus_shortname</span>: xxxxxxx<span class="comment">#如果你是disqus用户,输入你的shortname,即可开启评论服务.注意不是username.</span>
</code></pre><p><strong>语法注意:Hexo有一个强制语法要求,”:”后面必须要有个空格</strong></p>
<h3 id="安装第三方主题">安装第三方主题</h3>
<p>到<a href="https://github.com/hexojs/hexo/wiki/Themes," title="Hexo Theme" target="_blank" rel="external">Hexo Theme</a>浏览选择的主题,并复制主题github地址.</p>
<p>安装主题(这里以Alberta主题为例).</p>
<pre><code>git clone http<span class="variable">s:</span>//github.<span class="keyword">com</span>/ken8203/hexo-theme-alberta.git themes/alberta
</code></pre><p>然后配置<strong>_config.yml</strong>,选择新安装的主题.</p>
<pre><code><span class="tag">theme</span><span class="pseudo">:alberta</span>
</code></pre><p>配置theme/_config.yml,这里就不展开说明,详情请参考各个主题README.</p>
<p>更新主题</p>
<pre><code><span class="built_in">cd</span> themes/alberta
git pull
</code></pre><p>至此,我们就基本上完成了Hexo的一些常用配置,还有其他方面的需求,请参考.</p>
<p><a href="http://hexo.io/docs/,&#39;HEXO 官方DOCS&#39;" target="_blank" rel="external">Hexo 官方DOCS</a><br><a href="http://ibruce.info/2013/11/22/hexo-your-blog/&#39;Hexo你的博客&#39;" target="_blank" rel="external">Hexo你的博客</a><br><a href="http://zhaofei.tk/2014/11/30/jekyll_to_hexo/,&#39;更换博客系统——从jekyll到hexo&#39;" target="_blank" rel="external">更换博客系统——从jekyll到hexo</a></p>

      
    </div>
    <footer class="article-footer">
      
        <a href="http://blog.xiaolud.com/2015/01/19/Hexo_blog/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blog/">blog</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/github/">github</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hexo/">hexo</a></li></ul>

    </footer>
  </div>
  
</article>



  
  

</section>
        
      </div>
    </div>
    
    
<script>
  var disqus_shortname = 'xiaolud';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="http://libs.useso.com/js/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js" type="text/javascript"></script>


  </div>
</body>
</html>
